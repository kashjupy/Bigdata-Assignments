{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac377559",
   "metadata": {},
   "outputs": [],
   "source": [
    "1>Is it possible to use a script file to run HIVE queries? If so, include an example\n",
    "to back up your claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "To write the Hive Script the file should be saved with .sql extension. Open a terminal in your Cloudera CDH4 distribution and give the following command to create a Hive Script.\n",
    "Command: sudo gedit sample.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2>With a practical example, learn how to delete DBPROPERTY in Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can set key-value pairs in the DBPROPERTIES associated with a database using the ALTER DATABASE command. No other metadata about the database can be changed, including its name and directory location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efaa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "3>What exactly is an index in HIVE, and how do we make one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165047ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indexes are a pointer or reference to a record in a table as in relational databases. Indexing is a relatively new feature in Hive. In Hive, the index table is different than the main table. Indexes facilitate in making query execution or search operation faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4>Describe the architecture of HBase in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HBase architecture has 3 main components: HMaster, Region Server, Zookeeper. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    " – Architecture of HBase \n",
    "\n",
    "All the 3 components are described below: \n",
    " \n",
    "\n",
    "HMaster – \n",
    "The implementation of Master Server in HBase is HMaster. It is a process in which regions are assigned to region server as well as DDL (create, delete table) operations. It monitor all Region Server instances present in the cluster. In a distributed environment, Master runs several background threads. HMaster has many features like controlling load balancing, failover etc. \n",
    " \n",
    "Region Server – \n",
    "HBase Tables are divided horizontally by row key range into Regions. Regions are the basic building elements of HBase cluster that consists of the distribution of tables and are comprised of Column families. Region Server runs on HDFS DataNode which is present in Hadoop cluster. Regions of Region Server are responsible for several things, like handling, managing, executing as well as reads and writes HBase operations on that set of regions. The default size of a region is 256 MB. \n",
    " \n",
    "Zookeeper – \n",
    "It is like a coordinator in HBase. It provides services like maintaining configuration information, naming, providing distributed synchronization, server failure notification etc. Clients communicate with region servers via zookeeper. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "5>How to read and store a 1 GB CSV file into MongoDB using Spark. Create 1\n",
    "GB of data or gather it from the internet and show how it was implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "sql = SQLContext(sc)\n",
    "\n",
    "df = sql.read.csv(\"cal.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "Then write it to mongodb,\n",
    "\n",
    "df.write.format('com.mongodb.spark.sql.DefaultSource').mode('append')\\\n",
    "        .option('database',NAME).option('collection',COLLECTION_MONGODB).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/people.con?readPreference=primaryPreferred\" \\\n",
    "    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/people.con\" \\\n",
    "    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.3.0\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_spark = SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"myApp\") \\\n",
    "         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/people.con\") \\\n",
    "         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/people.con\") \\\n",
    "         .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(path = \"file:///home/user/Desktop/people.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"database\",\"people\").option(\"collection\", \"con\").save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
